---
title: "R Notebook"
output:
  word_document: default
  html_notebook: default
  pdf_document: default
  html_document:
    df_print: paged
---

# Problem 1
a) No

b) Yes: Classification

c) Yes: Regression 

d) No

e) Yes: Regression

f) Yes: Classification (if we are trying to predict if a given user will click)

g) Yes: Anomaly detection

h) No 

# Problem 2
a) Continuous, Quantitative, Ratio (Assuming that 4 is twice as bright as 2)

b) Continuous, Quantitative, Ratio

c) Discrete, Qualitative, Ordinal

d) Continuous, Quantitative, Interval (We can consider the 5 hour interval between 10AM and 3PM)

e) Discrete, Qualitative, Ordinal


# Problem 3
a)
```{r}
library(datasets)
data(iris)
sprintf("Number of datapoints: %d", nrow(iris))
sprintf("Number of features: %d", ncol(iris))
sprintf("Number of classes: %d", length(unique(iris$Species)))
```
b)
```{r}
summary(iris)
sprintf("Sepal.Length Std.Dev.: %f", sd(iris$Sepal.Length))
sprintf("Sepal.Width Std.Dev.: %f", sd(iris$Sepal.Width))
sprintf("Petal.Length Std.Dev.: %f", sd(iris$Petal.Length))
sprintf("Petal.Width Std.Dev.: %f", sd(iris$Petal.Width))
```
c)
```{r}
plot(iris$Sepal.Length ~ iris$Petal.Length,
      xlab = "Petal Length (cm)",
      ylab = "Sepal Length (cm)",
      pch = c(16, 17, 18)[unclass(iris$Species)], 
      main = "Iris Dataset",
      col = c("red", "green","blue")[unclass(iris$Species)],
      data = iris)

legend("topleft", 
        legend = c("setosa", "versicolor", "virginica"), 
        text.col = c("red", "green","blue"),
        col = c("red", "green","blue"), 
        pch = c(16, 17, 18))

```

# Problem 4
[Here is a graph of d1 and d2](https://www.desmos.com/calculator/zv8fqgvvzt)

Both d1 and d2 take values of 0 when s is 1 and converge to infinity as s approaches 0 (from the right). As such, both can be considered dissimilarity measures on the interval [0, infinity). d2 results in much large dissimilarity values.

# Problem 5
a) If the term only occurs in a single document, then the log term will be maximized, if the term occurs in every document then the log term will be 0

b) Common terms (the, a, because, that, etc.) that are likely to occur in most or all documents will have small values for tfij’. Uncommon words (diabetes, mahalanobis, dimensionality, etc.) will have larger values for tfij’. Words with a high tfij’ might be of greater interest when trying to make predictions about a given document.  

# Problem 6
a) Hamming = 4 ; Jaccard = 2/6 = ⅓

b) SMC = 1 - (Hamming Distance) / (Number of Bits)

c) Both Jaccard and Cosine measures ignore 0-0 matches. For our example with binary vectors, the numerator for both measures is the number of 1-1 matches. 

# Problem 7
Cosine = \(\frac{x \cdot y}{\sqrt{x \cdot x} * \sqrt{y \cdot y}}\)

Correlation = \(\frac{cov(x ,y)}{sd(x) * sd(y)}\)

Euclidian = \(\sqrt{\sum{(x_i - y_i) ^ 2}}\)

a) Cosine = 0 ; Correlation =  0 ; Euclidian = 2
b) Cosine = 0 ; Correlation = -1 ; Euclidian = 2 ; Jaccard = 0
